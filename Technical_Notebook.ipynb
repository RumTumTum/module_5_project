{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import Data_Clean_Process as dc\n",
    "import tn_helper as tn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.kernel_approximation import Nystroem, RBFSampler\n",
    "# from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Yelp Restaurants as Successful#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp holds contests for students and the public where they release a large amount of data, and then have the particpants compete to reach one of a few specialized goals, ranging from building social network graphs to image recognition. We are using this data to tackle our own question, can we predict whether a restaurant has a good rating (four or five stars) or a bad rating (one, two or three stars). Because of the size of the dataset, we are storing it on a google postgres server, so our first step is to fetch only the data that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = dc.download_data(\"K*%4t3VK0ab%gn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data cleaning was just formatting strings and filling in missing values without imputation, we clean before doing our train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "clean_data, numerical_features, categorical_features = dc.clean_data(\n",
    "    business_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = dc.tt_split(clean_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines ##\n",
    "We set up piplines to use for hyperparameter tuning for all of the models that we used to classify our yelp restaurants. The first step is to make a column transformer that applies the correct preprocessing steps to each feature column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features),\n",
    "    (OneHotEncoder(drop='first'), categorical_features)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the pipelines for the four models we tested, Logistic Regression, Random Forest, and two SVM methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('ct', preprocess),\n",
    "                    ('clf', LogisticRegression())])\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "                   'clf__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "                   }]\n",
    "\n",
    "log_results = pd.read_pickle('./CV_Results/Logistic_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('scl', preprocess),\n",
    "                    ('clf', RandomForestClassifier(oob_score=True, n_jobs=-1, verbose=1))])\n",
    "\n",
    "grid_params_rf = [{'clf__criterion': ['gini'],\n",
    "                   'clf__n_estimators': [300, 500],\n",
    "                   'clf__min_samples_leaf': [10, 13, 15, 17],\n",
    "                   'clf__max_depth': [20, 25, 30],\n",
    "                   'clf__max_features': ['log2', 'sqrt']\n",
    "                   }]\n",
    "\n",
    "rf_results = pd.read_pickle('./CV_Results/Random_Forest_final_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM ###\n",
    "Our two svm methods below use two different methods to approximate the radial kernel SVM using a linear SVC. One uses Nystroem method and the other uses a monte carlo method to approximate the radial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm_ny = Pipeline([('ct', preprocess),\n",
    "                        (\"feature_map\", Nystroem(gamma=.2)),\n",
    "                        ('clf', LinearSVC(verbose=1, max_iter=15000, dual=False))])\n",
    "\n",
    "grid_params_svm_ny = [{'feature_map__gamma': [.005, .01, .1],\n",
    "                       'clf__C': [3000, 5000, 7000],\n",
    "                       'clf__penalty': ['l1']\n",
    "                       }]\n",
    "\n",
    "svm_ny_results = pd.read_pickle('./CV_Results/SVM_Ny_RBF_CV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm_mc = Pipeline([('ct', preprocess),\n",
    "                        (\"feature_map\", RBFSampler()),\n",
    "                        ('clf', LinearSVC(verbose=1, max_iter=15000, dual=False))])\n",
    "\n",
    "grid_params_svm_mc = [{'feature_map__gamma': [.005, .01, .02],\n",
    "                       'clf__C': [1000, 2000, 3000],\n",
    "                       'clf__penalty': ['l1']\n",
    "                       }]\n",
    "\n",
    "svm_mc_results = pd.read_pickle('./CV_Results/SVM_MC_RBF_3_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This executes the grid search over all the choices of parameters that have been specified in the pipeline. To run the search, set estimator to the pipline you want, set param_grid to the corresponding param grid dictionary, set cv to the number of crossvalidation steps, and set scoring equal to the scoring metric you would like to use in crossvalidation.Depending on the choice of model and hyperparameters this grid search may take a very long time to run, so we have left it commented out. Below we have summaries of the grid search results for each model. The full results of the grid searches are saved as pickled dataframes in the CV_Results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch = GridSearchCV(estimator=pipe,\n",
    "#                           param_grid=grid_params_rf,\n",
    "#                           scoring='accuracy',\n",
    "#                           return_train_score=True,\n",
    "#                           cv=5)\n",
    "\n",
    "# gridsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a list of how the tuned versions of our models performed, with the list of hyper parameters, the mean cv test score and the mean fit time for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[rf_results, svm_mc_results, svm_ny_results, log_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>rf_results</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 10, 'clf__n_estimators': 300}</td>\n",
       "      <td>0.717585</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>6.973171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>svm_mc_results</td>\n",
       "      <td>{'clf__C': 1000, 'clf__penalty': 'l1', 'feature_map__gamma': 0.005}</td>\n",
       "      <td>0.704559</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>33.683355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>svm_ny_results</td>\n",
       "      <td>{'clf__C': 3000, 'clf__penalty': 'l1', 'feature_map__gamma': 0.005}</td>\n",
       "      <td>0.713564</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>54.919311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>log_results</td>\n",
       "      <td>{'clf__C': 1, 'clf__penalty': 'l1'}</td>\n",
       "      <td>0.709743</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>4.600864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              params  \\\n",
       "rf_results      {'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 10, 'clf__n_estimators': 300}   \n",
       "svm_mc_results                                                                   {'clf__C': 1000, 'clf__penalty': 'l1', 'feature_map__gamma': 0.005}   \n",
       "svm_ny_results                                                                   {'clf__C': 3000, 'clf__penalty': 'l1', 'feature_map__gamma': 0.005}   \n",
       "log_results                                                                                                      {'clf__C': 1, 'clf__penalty': 'l1'}   \n",
       "\n",
       "                mean_test_score  std_test_score  mean_fit_time  \n",
       "rf_results             0.717585        0.003375       6.973171  \n",
       "svm_mc_results         0.704559        0.008361      33.683355  \n",
       "svm_ny_results         0.713564        0.002129      54.919311  \n",
       "log_results            0.709743        0.003373       4.600864  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=150\n",
    "tn.top_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Random Forest model performed the best, but after tuning all of the models performed similarly in terms of accuracy. We decided to use Random forest as our model not only because it had the best performance, but because it had the second fastest mean fit time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=30, max_features='sqrt', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=10, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    max_depth=30, max_features='sqrt', min_samples_leaf=10, n_estimators=300, n_jobs=-1,oob_score=True)\n",
    "\n",
    "t_X_train=preprocess.fit_transform(X_train)\n",
    "t_X_test=preprocess.fit_transform(X_test)\n",
    "\n",
    "random_forest.fit(t_X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our final model on all of the testing data, and then verify it against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835083405329966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "random_forest.score(t_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166400850611377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing accuracy\n",
    "random_forest.score(t_X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score for our model on data that has not been used for hyperparameter tuning agrees with our crossvalidation results, so we can be pretty confident in the ability of our model to classify unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7371e74f4265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_X_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "importances=random_forest.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
